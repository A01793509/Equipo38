{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/A01793509/Equipo38/blob/main/Actividad_Semana_9_Equipo38.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Maestría en Inteligencia Artificial Aplicada**\n",
        "##**Curso: Inteligencia Artificial y Aprendizaje Automático**\n",
        "###Tecnológico de Monterrey\n",
        "###Profr: Luis Eduardo Falcón Morales\n",
        "\n",
        "## **Adtividad de la Semana 9**\n",
        "###**Taxonomía de Métricas de Clasificación**"
      ],
      "metadata": {
        "id": "LokPbfyA_jmk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nombres y matrículas de los integrantes del equipo:**\n",
        "\n",
        "*   Alberto Jose Garcia Porras (A01793509)\n",
        "*   Carlos Julio León Caicedo (A01793947)\n",
        "*   Luis Fernando Ríos Piedra (A00453954)\n",
        "*   Marco Antonio Vázquez Morales (A01793704)"
      ],
      "metadata": {
        "id": "2JVg-Ha7_2UV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resumen"
      ],
      "metadata": {
        "id": "JF5glsakAjtm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Garantizar una correcta evaluación del modelo de aprendizaje es clave para asegurar un adecuado reconocimiento de patrones, en ese sentido, existen dos aspectos clave a tener en cuenta para la evaluación, el primero basado en los datos estadísticos e intervalos de confianza del modelo, y el otro basado en la métrica utilizada para evaluarlo. Teniendo en cuenta que existen diferentes métricas de clasificación enfocadas en diversos tópicos del aprendizaje, es importante identificar la métrica adecuada para el modelo que estemos desarrollando.  Asimismo, dadas las características de las métricas, existe una clasificación que las agrupa por familia, generando básicamente las siguientes categorias:\n",
        "\n",
        "*   Métricas basadas en un umbral y una comprensión cualitativa del error (**Threshold**).\n",
        "*   Métricas basadas en una comprensión probabilistica del error (**Probabilistic**).\n",
        "*   Métricas basadas en que también se clasifica un modelo (**Rank**).\n",
        "\n",
        "Lo anterior nos lleva a otro interrogante, ¿Puedo seleccionar cualquier métrica para evaluar mi modelo?, precisamente esta es la respuesta que se plantea con el estudio, ya que no necesariamente se podrán extrapolar los resultados obtenidos a partir de una métrica aplicada a un modelo utilizando otras métricas de la misma familia, sobretodo cuando estamos lidiando con problemas multi-clase, clases desbalanceadas o conjuntos de datos pequeños. De igual forma, existen características presentes en algunas métricas que pueden generar sensibilidad en la obtención de los resultados generadas por las métricas que no poseen dichas características. Por lo anterior, es importante identificar la correlación existente entre las métricas que decidamos utilizar en nuestro modelo.\n",
        "Por otra parte, un aspecto a considerar es la diferencia que pueda existir entre los clasificadores y las buenas probabilidades, lo que nos lleva evaluar la calibración de las mismas, que no es otra cosa que el grado de aproximación de las probabilidades previstas versus las probabilidades reales. Sin embargo, esto nos provoca otro problema que tiene que ver con los segmentos o intervalos en que se divide un conjunto de datos, para lo que se ha experimentado superponerlos. \n",
        "\n",
        "Una de las métricas más usadas para evaluar una clasificación es la **precisión**, que es el grado de predicciones correctas de un modelo. \n",
        "\n",
        "**Área Bajo la Curva (AUC)** es otra de las métricas usadas, se define como la probabilidad de que un clasificador clasifique de manera positiva una instancia en vez de una negativa tomadas al azar. \n",
        "\n",
        "**Error Absoluto Medio (MAE)** muestra cuánto se desvian las predicciones de la probabilidad real. \n",
        "\n",
        "**Error Cuadrático Medio (MSE)** es una variante de MAE que penaliza fuertemente las debiaciones de la probabilidad real.\n",
        "\n",
        "Una primera conclusión que se llega en el artículo es que ninguna de las 18 métricas tiene un sí en el umbral de clase y en la calibraación o clasificación, esto es que no hay medida que combine el umbral con la probabilidad estimada. \n",
        "\n",
        "Una segunda conclusión es que al aplicar ruido al conjunto de datos, en este caso la precisión y otras medidas cualitativas son mejores cuando el ruido esta presente en el conjunto de datos. Por el contrario, estas son malas cuando el ruido es aplicado al aprendizaje.\n",
        "\n",
        "Finalmente, se observa que las diferentes métricas no pueden ser comparables, usando las correlaciones se encontró que estas suelen tener desempeño bajo entre las diversas medidas. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "daJsHg6kAme6"
      }
    }
  ]
}